{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Generation\n",
    "\n",
    "> for a given prefix we extract the end-term as shown in Figure 1. We match all the suffixes that\n",
    "> start with the end-term from our precomputed set. These selected suffixes are appended to the\n",
    "> prefix to generate synthetic suggestion candidates. For example, the prefix \"cheap flights fro\"\n",
    "> is matched with the suffix \"from seattle\" to generate the candidate \"cheap flights from seattle\".\n",
    "> Note that many of these synthetic suggestion candidates are likely to not have been observed by\n",
    "> the search engine before. We merge these synthetic suggestions with the set of candidates\n",
    "> selected from the list of historically popular queries. This combined set of candidates is used\n",
    "> for ranking as we will describe in Sec 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "POPULAR_SUFFIX_FILE = 'popular_suffix.csv'\n",
    "BASE_FILE = 'popular_query.csv'\n",
    "OUT_FILE = 'generated_candidate.csv'\n",
    "\n",
    "CHUNK_SIZE = 10000\n",
    "N_POPULAR_SUFFIX = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_df = pd.read_csv(POPULAR_SUFFIX_FILE, index_col=0, nrows=N_POPULAR_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_term(query):\n",
    "    \"\"\"\n",
    "    end_term('cheapest flight fro') = 'fro'\n",
    "    end_term('cheapest flight from') = 'from'\n",
    "    end_term('cheapest flight from ') = 'from '\n",
    "    end_term('cheapest flight from n') = 'n'\n",
    "    \"\"\"\n",
    "    if query.endswith(' '):\n",
    "        return query[query[:-1].rfind(' ')+1:]\n",
    "    else:\n",
    "        return query[query.rfind(' ')+1:]\n",
    "\n",
    "def match_end_term(end_term):\n",
    "    return list(suffix_df[suffix_df.ngram.str.startswith(end_term)].ngram)\n",
    "    \n",
    "def apply_end_term(row):\n",
    "    candidates = []        \n",
    "    query = original_query = row.iloc[0].Query\n",
    "    \n",
    "    while query.find(' ') != -1: # There is more than one word\n",
    "        term = end_term(query)\n",
    "        suffixes = match_end_term(term)\n",
    "                \n",
    "        for suffix in suffixes:\n",
    "            new_query = query + suffix[len(term):]\n",
    "            \n",
    "            if new_query != original_query or True:\n",
    "                candidates.append({\n",
    "                    'Prefix': query,\n",
    "                    'Suffix': suffix,\n",
    "                    'Query': new_query\n",
    "                })\n",
    "        \n",
    "        query = query[:-1]\n",
    "    \n",
    "    return pd.DataFrame(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_FILE, 'w') as the_file: the_file.write('Prefix,Query,Suffix\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'Index': 'int64',\n",
    "    'AnonID': 'str',\n",
    "    'Query': 'str',\n",
    "    'QueryTime': 'str',\n",
    "    'ItemRank': 'str',\n",
    "    'ClickURL': 'str',\n",
    "}\n",
    "\n",
    "# only load index and Query\n",
    "chunks = pd.read_csv(BASE_FILE, index_col=0, dtype=dtypes, usecols=[0, 1], low_memory=False, chunksize=CHUNK_SIZE)\n",
    "\n",
    "# Count the number of chunks in this file\n",
    "num_chunks = int(sum(1 for row in open(BASE_FILE, 'r')) / CHUNK_SIZE) + 1\n",
    "chunk_id = iter(range(1, num_chunks+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in chunks:\n",
    "    print(\"Processing chunk {} of {}\".format(next(chunk_id), num_chunks), end=\"\\r\")\n",
    "    # Any empty query is not interesting\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # 1. Apply suffix_ngrams, creates a list of ngrams for each row\n",
    "    # 2. Apply pd.Series, creates a series for this list\n",
    "    # 3. Merge the applied series with the dataframe\n",
    "    # 3. Drop the Query column, we don't need it anymore\n",
    "    # 4. Reset the index, make it available for selection\n",
    "    # 5. Melt with the index as id, this flattens the ngrams list\n",
    "    # 6. Drop the variable and index columns, they are not interesting anymore\n",
    "    # 7. Drop any empty values (any rows with less than NUMBER_OF_NGRAMS ngrams.\n",
    "    df2 = df.groupby(df.columns.tolist(), group_keys=False).apply(apply_end_term)\n",
    "    \n",
    "    df2.to_csv(OUT_FILE, mode='a', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
